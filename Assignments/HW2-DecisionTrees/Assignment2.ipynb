{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This assignment may be worked individually or in pairs. Enter your name/s here:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danica Padlan, Ashley Yude\n",
    "\n",
    "#Don't need to do this\n",
    "#Feature Engineering Notes\n",
    "#-calc average of columns 2-7 for avg MA result detection\n",
    "#-maybe calc avg of columns 8-15 for avg normalized exudate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 2: Decision Trees\n",
    "\n",
    "In this assignment we'll implement the Decision Tree algorithm to classify patients as either having or not having diabetic retinopathy. For this task we'll be using the Diabetic Retinopathy data set, which contains features from the Messidor image set to predict whether an image contains signs of diabetic retinopathy or not. This dataset has `1150` records and `20` attributes (some categorical, some continuous). You can find additional details about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Information:\n",
    "\n",
    "0) The binary result of quality assessment. 0 = bad quality 1 = sufficient quality.\n",
    "\n",
    "1) The binary result of pre-screening, where 1 indicates severe retinal abnormality and 0 its lack. \n",
    "\n",
    "2-7) The results of MA detection. Each feature value stand for the number of MAs found at the confidence levels alpha = 0.5, . . . , 1, respectively. \n",
    "\n",
    "8-15) contain the same information as 2-7) for exudates. However, as exudates are represented by a set of points rather than the number of pixels constructing the lesions, these features are normalized by dividing the \n",
    "number of lesions with the diameter of the ROI to compensate different image sizes. \n",
    "\n",
    "16) The euclidean distance of the center of the macula and the center of the optic disc to provide important information regarding the patient's condition. This feature is also normalized with the diameter of the ROI.\n",
    "\n",
    "17) The diameter of the optic disc. \n",
    "\n",
    "18) The binary result of the AM/FM-based classification.\n",
    "\n",
    "19) Class label. 1 = contains signs of Diabetic Retinopathy, 0 = no signs of Diabetic Retinopathy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation: \n",
    "The function prototypes are given to you, please don't change those. You can add additional helper functions if needed. \n",
    "\n",
    "*Suggestion:* The dataset is substantially big, for the purpose of easy debugging, work with a subset of the data and test your decision tree implementation on that.\n",
    "\n",
    "#### Notes:\n",
    "Parts of this assignment will be **autograded** so a couple of caveats :-\n",
    "- Entropy is calculated using log with base 2, `math.log2(x)`.\n",
    "- For continuous features ensure that the threshold value lies exactly between 2 values. For example, if for feature 2 the best split occurs between 10 and 15 then the threshold value will be set as 12.5. For binary features [0/1] the threshold value will be 0.5.\n",
    "- All values < `thresh_val` go to the left child and all values >= `thresh_val` go to the right child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Headers\n",
    "# You are welcome to add additional headers if you wish\n",
    "# EXCEPT for scikit-learn... You may NOT use scikit-learn for this assignment!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPoint:\n",
    "    def __str__(self):\n",
    "        return \"< \" + str(self.label) + \": \" + str(self.features) + \" >\"\n",
    "    def __init__(self, label, features):\n",
    "        self.label = label # the classification label of this data point\n",
    "        self.features = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Read data from a CSV file. Put it into a list of `DataPoints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    data = []\n",
    "    df = pd.read_csv(filename, header=None, names=['Quality Assessment Result', 'Pre-screening Result', 'MA Detection Result 1',\n",
    "                                                  'MA Detection Result 2', 'MA Detection Result 3', 'MA Detection Result 4', 'MA Detection Result 5',\n",
    "                                                  'MA Detection Result 6', 'Normalized Exudate 1', 'Normalized Exudate 2', 'Normalized Exudate 3',\n",
    "                                                  'Normalized Exudate 4', 'Normalized Exudate 5', 'Normalized Exudate 6', 'Normalized Exudate 7',\n",
    "                                                  'Normalized Exudate 8', 'Euclidean Distance', 'Optic Disc Diameter', 'AM/FM Result', 'Has Diabetic Retinopathy'])\n",
    "    \n",
    "    #loop through all rows in data frame\n",
    "    for x in range (df.shape[0]):\n",
    "        \n",
    "        #create Data Point and add to list\n",
    "        #before had 1:19\n",
    "        new_DataPoint = DataPoint(df.iloc[x,19], df.iloc[x, 0:19])\n",
    "        data.append(new_DataPoint)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    is_leaf = True          # boolean variable to check if the node is a leaf\n",
    "    feature_idx = None      # index that identifies the feature\n",
    "    thresh_val = None       # threshold value that splits the node\n",
    "    prediction = None       # prediction class (only valid for leaf nodes)\n",
    "    left_child = None       # left TreeNode (all values < thresh_val)\n",
    "    right_child = None      # right TreeNode (all values >= thresh_val)\n",
    "    \n",
    "    def printTree(self, level=0):    # for debugging purposes\n",
    "        if self.is_leaf:\n",
    "            print ('-'*level + 'Leaf Node:      predicts ' + str(self.prediction))\n",
    "        else:\n",
    "            print ('-'*level + 'Internal Node:  splits on feature ' \n",
    "                   + str(self.feature_idx) + ' with threshold ' + str(self.thresh_val))\n",
    "            self.left_child.printTree(level+1)\n",
    "            self.right_child.printTree(level+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Implement the function `make_prediction` that takes the decision tree root and a `DataPoint` instance and returns the prediction label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(tree_root, data_point):\n",
    "    \n",
    "    #base case\n",
    "    if tree_root.is_leaf:\n",
    "        return tree_root.prediction\n",
    "    \n",
    "    #check left/right node based on comparison to thresh value\n",
    "    if (data_point.features.iloc[tree_root.feature_idx] < tree_root.thresh_val):\n",
    "        return make_prediction(tree_root.left_child, data_point)\n",
    "    else:\n",
    "        return make_prediction(tree_root.right_child, data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing make predict\n",
    "# data = get_data(\"messidor_features.txt\")\n",
    "# # print(len(data[0].features))\n",
    "\n",
    "# #isDiabRetin = TreeNode(True, None, None, 1, None, None)\n",
    "# isDiabRetin = TreeNode()\n",
    "# isDiabRetin.is_leaf = True\n",
    "# isDiabRetin.feature_idx = None      \n",
    "# isDiabRetin.thresh_val = None       \n",
    "# isDiabRetin.prediction = 1       \n",
    "# isDiabRetin.left_child = None       \n",
    "# isDiabRetin.right_child = None\n",
    "\n",
    "# #notDiabRetin = TreeNode(True, None, None, 0, None, None)\n",
    "# notDiabRetin = TreeNode()\n",
    "# notDiabRetin.is_leaf = True\n",
    "# notDiabRetin.feature_idx = None      \n",
    "# notDiabRetin.thresh_val = None       \n",
    "# notDiabRetin.prediction = 0       \n",
    "# notDiabRetin.left_child = None       \n",
    "# notDiabRetin.right_child = None\n",
    "\n",
    "# #root = TreeNode(False, 0, 1, None, notDiabRetin, isDiabRetin)\n",
    "# root = TreeNode()\n",
    "# root.is_leaf = False\n",
    "# root.feature_idx = 0 \n",
    "# root.thresh_val = 1       \n",
    "# root.prediction = None      \n",
    "# root.left_child = isDiabRetin       \n",
    "# root.right_child = notDiabRetin\n",
    "\n",
    "# print(make_prediction(root, data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Implement the function `split_dataset` given an input data set, a `feature_idx` and the `threshold` for the feature. `left_split` will have all values < `threshold` and `right_split` will have all values >= `threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, feature_idx, threshold):\n",
    "    left_split = []\n",
    "    right_split = []\n",
    "    \n",
    "    #Loop through data and split based on comparison to threshold\n",
    "    for x in range (len(data)):\n",
    "        \n",
    "        if (data[x].features.iloc[feature_idx] < threshold):\n",
    "            left_split.append(data[x])\n",
    "        else:\n",
    "            right_split.append(data[x])\n",
    "                    \n",
    "    return (left_split, right_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Implement the function `calc_entropy` to return the entropy of the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(data):\n",
    "    #data is an array\n",
    "    \n",
    "    #Special case\n",
    "    if (len(data) == 0):\n",
    "        return 0\n",
    "    \n",
    "    entropy = 0.0\n",
    "    yes_count = 0\n",
    "    no_count = 0\n",
    "    \n",
    "    #Increment yes & no counts for each datapoint's label\n",
    "    for cur_datapoint in data:\n",
    "        \n",
    "        if cur_datapoint.label == 1:\n",
    "            yes_count+= 1\n",
    "        else:\n",
    "            no_count+= 1\n",
    "    \n",
    "    #Create fractions based on yes/no counts\n",
    "    yes_frac = (yes_count / len(data)) if yes_count > 0 else 0\n",
    "    no_frac = (no_count / len(data)) if no_count > 0 else 0\n",
    "    \n",
    "    #Calculate Entropy\n",
    "    if (yes_frac == 0):\n",
    "        entropy = - ((no_frac)*log2(no_frac))\n",
    "    elif (no_frac == 0):\n",
    "        entropy = - ((yes_frac)*log2(yes_frac))\n",
    "    else:\n",
    "        entropy = -((yes_frac)*log2(yes_frac)) - ((no_frac)*log2(no_frac))\n",
    "    \n",
    "    return abs(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing calc_entropy\n",
    "# # 2, 1, 3, 7, 10, 11\n",
    "# temp = []\n",
    "# data = get_data(\"messidor_features.txt\")\n",
    "# temp.append(data[2])\n",
    "# temp.append(data[1])\n",
    "# temp.append(data[3])\n",
    "# temp.append(data[7])\n",
    "# temp.append(data[10])\n",
    "# temp.append(data[11])\n",
    "\n",
    "# temp = np.array(temp)\n",
    "\n",
    "# print(calc_entropy(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Implement the function `calc_best_threshold` which returns the best information gain and the corresponding threshold value for one feature at `feature_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_best_threshold(data, feature_idx):\n",
    "    best_info_gain = 0.0\n",
    "    best_thresh = 0.0\n",
    "    \n",
    "    #keep track of datapoints in ascending order of feature values\n",
    "    #take in values of specific feature in data list\n",
    "    #[feature sorted by, class label]\n",
    "    rows, cols = (len(data), 2)\n",
    "    \n",
    "    #creates matrix\n",
    "    sorted_features = [ [0]*cols for i in range(rows)]\n",
    "    \n",
    "    #fills in matrix by feature value by feature index and class label\n",
    "    for x in range (len(data)): \n",
    "        sorted_features[x][0] = data[x].features.iloc[feature_idx]\n",
    "        sorted_features[x][1] = data[x].label\n",
    "    \n",
    "    #sorted by feature values in ascending order\n",
    "    \n",
    "    sorted_features = np.array(sorted_features)\n",
    "    sorted_features = sorted_features[sorted_features[:,0].argsort()]\n",
    "    \n",
    "    #loop and find best threshold combo\n",
    "    for x in range (len(sorted_features) - 1):\n",
    "        \n",
    "        #checks if data next to each other has different labels\n",
    "        if sorted_features[x][1] != sorted_features[x+1][1]: \n",
    "            \n",
    "            #also works for binary values!\n",
    "            #main problem is threshold isn't ~X.5, we get a whole number\n",
    "            mid_thresh = (sorted_features[x][0] + sorted_features[x+1][0]) / 2\n",
    "            \n",
    "            #use split method and pass threshold \n",
    "            l_and_r = split_dataset(data, feature_idx, mid_thresh)\n",
    "            \n",
    "            left = l_and_r[0]\n",
    "            right = l_and_r[1]\n",
    "            \n",
    "            #get entropy values for parent, left, and right data\n",
    "            left_ent = calc_entropy(left)\n",
    "            right_ent = calc_entropy(right)\n",
    "            parent_ent = calc_entropy(data)\n",
    "            \n",
    "            #calculate class labels from both splits  by sending it in to calc entropy\n",
    "            cur_gain = parent_ent - (((len(left) / len(data)) * left_ent) + ((len(right) / len(data)) * right_ent)) \n",
    "            \n",
    "            #checking for highest gain \n",
    "            if cur_gain > best_info_gain:\n",
    "                best_info_gain = cur_gain\n",
    "                best_thresh = mid_thresh\n",
    "    \n",
    "    return (best_info_gain, best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tested!\n",
    "#testing calc_best_threshold()\n",
    "# feat = 2;\n",
    "# test_data = []\n",
    "# test_data.append(data[0])\n",
    "# test_data.append(data[1])\n",
    "# test_data.append(data[2])\n",
    "# test_data.append(data[3])\n",
    "# test_data.append(data[4])\n",
    "# test_data.append(data[5])\n",
    "# result = calc_best_threshold(test_data, feat)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Implement the function `identify_best_split` which returns the best feature to split on for an input dataset, and also returns the corresponding threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_best_split(data):\n",
    "    \n",
    "    #special case \n",
    "    if len(data) < 2:\n",
    "        return (None, None)\n",
    "    best_feature = 0.0\n",
    "    best_thresh = 0.0\n",
    "    best_gain = 0.0\n",
    "\n",
    "    #loop through all features\n",
    "    for x in range (len(data[0].features)):\n",
    "        \n",
    "        #get high gain and best thresh\n",
    "        cur_gain, cur_thresh = calc_best_threshold(data, x)\n",
    "\n",
    "        #save thresh and feature based on highest gain\n",
    "        if cur_gain > best_gain:\n",
    "            best_gain = cur_gain\n",
    "            best_thresh = cur_thresh\n",
    "            best_feature = x\n",
    "\n",
    "    #return the one with best gain\n",
    "    return (best_feature, best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tested!\n",
    "#testing identify_best_split\n",
    "# feat = 2;\n",
    "# test_data = []\n",
    "# test_data.append(data[0])\n",
    "# test_data.append(data[1])\n",
    "# test_data.append(data[2])\n",
    "# test_data.append(data[3])\n",
    "# test_data.append(data[4])\n",
    "# test_data.append(data[5])\n",
    "# result = calc_best_threshold(test_data, feat)\n",
    "# print(result)\n",
    "# identify_best_split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Implement the function `create_leaf_node` which returns a `TreeNode` with `is_leaf=True` and `prediction` set to whichever classification occurs most in the dataset at this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leaf_node(data):\n",
    "    \n",
    "    # Making a leaf node & filling in variables\n",
    "    leaf = TreeNode()\n",
    "    leaf.is_leaf = True\n",
    "    \n",
    "    yes = 0\n",
    "    no = 0\n",
    "    for x in data:\n",
    "        if x.label == 1:\n",
    "            yes += 1\n",
    "        else:\n",
    "            no += 1\n",
    "            \n",
    "    #update prediction based on yes/no counts\n",
    "    leaf.prediction = 1 if yes > no else 0\n",
    "\n",
    "    return leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tested!\n",
    "#TODO: test create_leaf_node\n",
    "# test_data = []\n",
    "# test_data.append(data[0])\n",
    "# test_data.append(data[1])\n",
    "# test_data.append(data[2])\n",
    "# test_data.append(data[3])\n",
    "# test_data.append(data[4])\n",
    "# test_data.append(data[5])\n",
    "\n",
    "# leeef = create_leaf_node(test_data)\n",
    "# print(leeef.prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Implement the `create_decision_tree` function. `max_levels` denotes the maximum height of the tree (for example if `max_levels = 1` then the decision tree will only contain the leaf node at the root. [Hint: this is where the recursion happens.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_leaf(data):\n",
    "\n",
    "    yes_counter = 0\n",
    "    no_counter = 0\n",
    "    \n",
    "    #Loop through datapoints & increment yes/no counters based on label\n",
    "    for x in data:\n",
    "        if x.label == 1:\n",
    "            yes_counter += 1\n",
    "        else:\n",
    "            no_counter += 1\n",
    "    \n",
    "    #Node is leaf if all yes's or all no's\n",
    "    return True if ((yes_counter == len(data)) or (no_counter == len(data))) else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_build(data, max_levels, cur_levels):\n",
    "    \n",
    "    #base case\n",
    "    if (cur_levels >= max_levels) or (is_leaf(data)):\n",
    "        return create_leaf_node(data)\n",
    "    else:\n",
    "        best_feature, best_thresh = identify_best_split(data)\n",
    "    \n",
    "        #create TreeNode\n",
    "        curr_node = TreeNode()\n",
    "        curr_node.is_leaf = False\n",
    "        curr_node.feature_idx = best_feature     \n",
    "        curr_node.thresh_val = best_thresh\n",
    "        \n",
    "        #split data for left and right child\n",
    "        left_data, right_data = split_dataset(data, best_feature, best_thresh)\n",
    "    \n",
    "        #set left and right child nodes by starting recursive call\n",
    "        left_node = helper_build(left_data, max_levels, cur_levels + 1)\n",
    "        right_node = helper_build(right_data, max_levels, cur_levels + 1)\n",
    "        \n",
    "        #set curr_nodes left and right node\n",
    "        curr_node.left_child = left_node\n",
    "        curr_node.right_child = right_node\n",
    "        \n",
    "        return curr_node;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decision_tree(data, max_levels):\n",
    "\n",
    "    cur_level = 1\n",
    "\n",
    "    #if max levels == curlevel, root node is a leaf node\n",
    "    if cur_level == max_levels:\n",
    "        return create_leaf_node(data)\n",
    "    \n",
    "    #Get best feature & its thresh\n",
    "    best_feature, best_thresh = identify_best_split(data)\n",
    "    \n",
    "    #Create the root node & update values\n",
    "    root_node = TreeNode()\n",
    "    root_node.is_leaf = False\n",
    "    root_node.feature_idx = best_feature     \n",
    "    root_node.thresh_val = best_thresh    \n",
    "    \n",
    "    #Split the data set based on thresh of chosen \"best feature\"\n",
    "    left_data, right_data = split_dataset(data, best_feature, best_thresh)\n",
    "    \n",
    "    #set left and right child nodes by starting recursive call\n",
    "    left_node = helper_build(left_data, max_levels, cur_level + 1)\n",
    "    right_node = helper_build(right_data, max_levels, cur_level + 1)\n",
    "    \n",
    "    #Update root node's child values based on recursive call return value\n",
    "    root_node.left_child = left_node\n",
    "    root_node.right_child = right_node\n",
    "\n",
    "    return root_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #tested!\n",
    "# #TODO: test create_leaf_node\n",
    "# test_data = []\n",
    "# test_data.append(data[0])\n",
    "# test_data.append(data[1])\n",
    "# test_data.append(data[2])\n",
    "# test_data.append(data[3])\n",
    "# test_data.append(data[4])\n",
    "# test_data.append(data[5])\n",
    "# test_data.append(data[6])\n",
    "# test_data.append(data[7])\n",
    "# test_data.append(data[8])\n",
    "# test_data.append(data[9])\n",
    "# test_data.append(data[10])\n",
    "# test_data.append(data[11])\n",
    "# test_data.append(data[12])\n",
    "# test_data.append(data[13])\n",
    "# test_data.append(data[14])\n",
    "# test_data.append(data[15])\n",
    "# test_data.append(data[16])\n",
    "# test_data.append(data[17])\n",
    "# test_data.append(data[18])\n",
    "# test_data.append(data[19])\n",
    "# test_data.append(data[20])\n",
    "\n",
    "\n",
    "# # leeef = create_leaf_node(test_data)\n",
    "# root = create_decision_tree(test_data, 5)\n",
    "# root.printTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEST CODE\n",
    "\n",
    "## SMALL DATASET\n",
    "# print('\\nSMALL DATASET')\n",
    "# temp_data = get_data(\"sample_train.txt\")\n",
    "# print(f'Length of dataset: {len(temp_data)}')\n",
    "\n",
    "# print(f'Entropy of dataset: {calc_entropy(temp_data)}')\n",
    "\n",
    "# best_gain, best_thresh = calc_best_threshold(temp_data, 3)\n",
    "# print(\"Best thresh:\", best_thresh)\n",
    "# print(\"Best gain:\", best_gain)\n",
    "\n",
    "# feat, thresh = identify_best_split(temp_data)\n",
    "# print(f\"Best split: feature_index={feat}, thresh={thresh}\")\n",
    "\n",
    "# ## FULL DATASET\n",
    "# print('\\nFULL DATASET')\n",
    "# full_data = get_data(\"messidor_features.txt\")\n",
    "# print(f'Length of dataset: {len(full_data)}')\n",
    "\n",
    "# print(f'Entropy of dataset: {calc_entropy(full_data)}')\n",
    "\n",
    "# best_gain, best_thresh = calc_best_threshold(full_data, 3)\n",
    "# print(\"Best thresh:\", best_thresh)\n",
    "# print(\"Best gain:\", best_gain)\n",
    "\n",
    "# feat, thresh = identify_best_split(full_data)\n",
    "# print(f\"Best split: feature_index={feat}, thresh={thresh}\")\n",
    "\n",
    "# print(\"Create Tree:\")\n",
    "# temp_tree = create_decision_tree(temp_data, 10)\n",
    "# temp_tree.printTree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Given a test set, the function `calc_accuracy` returns the accuracy of the classifier. You'll use the `make_prediction` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(tree_root, data):\n",
    "    correct = 0;\n",
    "    \n",
    "    #loop through all data and count number of times predicted value is correct\n",
    "    for x in data:\n",
    "        predicted_value = make_prediction(tree_root, x)\n",
    "        actual_value = x.label\n",
    "        \n",
    "        if predicted_value == actual_value:\n",
    "            correct +=1\n",
    "\n",
    "    #returns correct values / total data\n",
    "    return (correct / len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #tested :,(\n",
    "# test_data = data\n",
    "# print(calc_accuracy(root, test_data))\n",
    "#print(calc_accuracy(root, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. Keeping the `max_levels` parameter as 10, use 5-fold cross validation to measure the accuracy of the model. Print the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 920\n",
      "Test set size    : 230\n",
      "The accuracy on the test set is  63.04347826086957\n",
      "Training set size: 920\n",
      "Test set size    : 230\n",
      "The accuracy on the test set is  63.04347826086957\n",
      "Training set size: 920\n",
      "Test set size    : 230\n",
      "The accuracy on the test set is  67.82608695652173\n",
      "Training set size: 920\n",
      "Test set size    : 230\n",
      "The accuracy on the test set is  63.47826086956522\n",
      "Training set size: 920\n",
      "Test set size    : 230\n",
      "The accuracy on the test set is  64.34782608695652\n",
      "Avg error over 5-fold: 64.34782608695652\n"
     ]
    }
   ],
   "source": [
    "# edit the code here - this is just a sample to get you started\n",
    "import time\n",
    "\n",
    "d = get_data(\"messidor_features.txt\")\n",
    "\n",
    "acc_avg_total = 0.0\n",
    "\n",
    "#1150/5 = 230 data per fold\n",
    "\n",
    "start_test = 0 #inclusive range val\n",
    "end_test = 230 #exclusive range val\n",
    "\n",
    "#cross validation loop for 5-fold\n",
    "for x in range (5):\n",
    "    \n",
    "    # the timer is just for fun! you will NOT be graded on runtime\n",
    "    # start = time.time()\n",
    "    \n",
    "    # partition data into train_set and test_set\n",
    "    train_set = d[0:start_test]\n",
    "    train_set.extend(d[end_test:len(d)])\n",
    "    test_set = d[start_test:end_test]\n",
    "\n",
    "    print ('Training set size:', len(train_set))\n",
    "    print ('Test set size    :', len(test_set))\n",
    "\n",
    "    # create the decision tree\n",
    "    tree = create_decision_tree(train_set, 10)\n",
    "\n",
    "    #end = time.time()\n",
    "    #print ('Time taken:', end - start)\n",
    "\n",
    "    # calculate the accuracy of the tree\n",
    "    accuracy = calc_accuracy(tree, test_set)\n",
    "    print ('The accuracy on the test set is ', str(accuracy * 100.0))\n",
    "    #t.printTree()\n",
    "    \n",
    "    #update start and end values\n",
    "    start_test += 230\n",
    "    end_test += 230\n",
    "    \n",
    "    acc_avg_total += accuracy\n",
    " \n",
    "# print acc_avg_total / 5 overall accuracy   \n",
    "print('Avg error over 5-fold:', (acc_avg_total / 5) * 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
